import pathlib
from PileEvaluationTasks import EvaluationTask
import argparse

def main():

    parser = argparse.ArgumentParser()
    # parser.add_argument("generated_strings", type=str, help="Strings generated by the model to search for.")
    # parser.add_argument("output_file", type=str, help="The output file to save the evaluation results.")
    parser.add_argument("start", type=int, help="index to start searching at.")
    parser.add_argument("end", type=int, help="Index to stop searching at.")
    parser.add_argument("--num_workers", type=int, default=None, help="Number of workers to use for multiprocessing.")
    parser.add_argument("--results_dir", type=str, default="results", help="Directory to save results to.")
    args = parser.parse_args()

    # generated_strings = args.generated_strings
    # output_file = args.output_file
    start_index = args.start
    end_index = args.end

    # start_index = 1
    # end_index = 2
    gold_strings = 'human_eval'

    # create results dir if not exists
    pathlib.Path(args.results_dir).mkdir(parents=True, exist_ok=True)
    output_file = f'{args.results_dir}/{start_index}_{end_index}.jsonl'

    print("Starting evaluation on dataset: {}".format(gold_strings))
    print("Saving results to: {}".format(output_file))
    print("Searching the range [{}, {}]".format(start_index, end_index))

    print(start_index, end_index)
    # return
    task = EvaluationTask(gold_strings, output_file, start_index, end_index, num_workers=args.num_workers)
    task.start_scoring()

if __name__ == '__main__':
    main()